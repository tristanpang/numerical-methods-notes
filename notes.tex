\documentclass[11pt, a4paper]{article}

%% Packages
\usepackage{amsmath}
\usepackage{amsfonts,amsthm,amssymb,enumerate}
\usepackage{pgf,tikz}
\usepackage{graphicx}
\graphicspath{{../figures/}}
\usepackage[hidelinks]{hyperref}

%% Page style
\setlength{\parindent}{0pt}
\setlength\parskip{.7em plus 0.1em minus 0.2em}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\footrulewidth}{0.4pt}
\cfoot{Page \thepage\ of \pageref{lastpage}}
\rhead{Draft \textit{\number\day/\number\month/\number\year}}
\lhead{\leftmark}
\fancyheadoffset{0cm}

\usepackage[headsep=.5cm,headheight=14.5pt,margin=3cm]{geometry}
%\usepackage{showframe}

%% Shortcuts
\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\KK}{\mathbb{K}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\inv}{^{-1}}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\phi}{\varphi}

%% Theorems
\newtheoremstyle{break}
{\topsep}{\topsep}%
{}{}%
{\bfseries}{}%
{\newline}{}%

\theoremstyle{break}
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{propdefn}[thm]{Proposition/Definition}
\newtheorem{lemdefn}[thm]{Lemma/Definition}
\newtheorem{eg}[thm]{Example}
\newtheorem{ex}[thm]{Exercise}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{note}[thm]{Note}
\newtheorem{defn}[thm]{Definition}
\newtheorem{rmk}[thm]{Remark}
\newtheorem{re}[thm]{Results}
\newtheorem{code}[thm]{Code}
\newenvironment{prf}[1][\proofname]{%
	\begin{proof}[#1]$ $\par\nobreak\ignorespaces
	}{%
	\end{proof}
}

%% Code
\usepackage{listings}
\lstset{language=Python,upquote=true, 
	numbers=left, stepnumber=1, 
	basicstyle=\small\ttfamily,columns=fullflexible,tabsize=5, xleftmargin=.25in, 	showstringspaces=false,breaklines=true,keepspaces}

%% Magic
\makeatletter
\newcommand*{\perm}[1]{\text{$(\@for\tmp:=#1\do{{\tmp}\;}\hspace{-.277em})$}}%permutation
\makeatother
\makeatletter
\newcommand*{\permcom}[1]{\text{$(#1)$}}%permutation
\makeatother
\newcommand*{\ideal}[1]{\text{$\langle#1\rangle$}}%ideal
\newcommand*{\Ideal}[1]{\text{$\left\langle#1\right\rangle$}}%ideal
\def\matrixtwooneHelp(#1,#2){\begin{pmatrix}#1\\#2\end{pmatrix}}%matrix
\def\matrixtwoHelp(#1,#2,#3,#4){\begin{pmatrix}#1&#2\\#3&#4\end{pmatrix}}%matrix
\def\matrixthreeHelp(#1,#2,#3,#4,#5,#6,#7,#8,#9){\begin{pmatrix}#1&#2&#3\\#4&#5&#6\\#7&#8&#9 \end{pmatrix}}%matrix
\newcommand*{\matrixtwo}[1]{\matrixtwoHelp(#1)}
\newcommand*{\matrixtwoone}[1]{\matrixtwooneHelp(#1)}
\newcommand*{\matrixthree}[1]{\matrixthreeHelp(#1)}
\newcommand*{\size}[1]{|#1|}%size of group
\newcommand*{\Size}[1]{{\left|#1\right|}}%big size
\newcommand*{\ind}[2]{|#1:#2|}
\newcommand*{\set}[1]{\{#1\}}%set 
\newcommand*{\Set}[1]{\left\{#1\right\}}%set 
\newcommand*{\paren}[1]{(#1)}%set 
\newcommand*{\Paren}[1]{\left(#1\right)}%set 
\newcommand{\cross}{\times}
\newcommand{\rcross}{\rtimes}
\newcommand{\lcross}{\ltimes}
\newcommand{\id}{\mathsf{id}}


%% Specific commands
\newcommand{\ve}{\mathbf} % Vectors
\newcommand{\dt}{\Delta t}
\newcommand{\dx}{\Delta x}
\newcommand{\der}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dder}[2]{\frac{d #1}{d #2}}
\newcommand{\ddder}[3][2]{\frac{d^#1 #2}{d #3^#1}}
\newcommand{\eval}[1]{\big\rvert_{#1}}
\newcommand{\Eval}[1]{\bigg\rvert_{#1}}
\begin{document}
\begin{center}
{\LARGE Numerical Methods}\\
{\large Compiled by Tristan Pang}\\
{\the\month/\the\year}
\end{center}
These notes are based on the Oxford Numerical Methods course taught by David Marshall (2023 for the NERC DTP) with additional information from LeVeque \cite{leveque_finite_2007}.

These are rough notes only. A polished version may or may not be completed. Please direct all typos to me. The \href{https://github.com/tristanpang/numerical-methods-notes}{GitHub repo}\footnote{https://github.com/tristanpang/numerical-methods-notes} contains \LaTeX\ source, Python scripts for figures, and other useful Python things.

\tableofcontents



\section{Root finding}
Consider a sufficiently smooth function $f(x)$. If $f$ is a quadratic polynomial, we may find the zeros of $f$ using the quadratic formula, but for degrees 5 or larger, there exists no general formula for the zeros (Abelâ€“Ruffini theorem). In general, finding an $x^*$ such that $f(x^*)=0$ cannot be computed exactly. Instead one must employ numerical root finding algorithms. Common methods include the bisection method and Newton's method.

\subsection{Bisection method}
To find a zero $x^*$ of $f$, the bisection method takes two initial guesses $a$ and $b$ such that $a<0$ and $b\geq0$. IVT guarantees a zero between the two guesses. Calculate the midpoint \[c=\frac{a+b}2.\] If $f(c)<0$, replace $a$ with $c$; otherwise replace $b$ with $c$. Continue iterating until convergence is observed as shown in Figure~\ref{fig:bisection}.

\begin{figure}\centering
	\includegraphics[width=0.7\textwidth]{example-image-golden}
	\caption{Bisection method}\label{fig:bisection}
\end{figure}

The error at the first iteration is \begin{align*}
\epsilon_1 &= |c - x^*| \\&= \Size{\frac{a-x^*}{2}+\frac{b-x^*}{2}}\\&= \Bigg|\Size{\frac{a-x^*}{2}}-\Size{\frac{b-x^*}{2}}\Bigg| \\&\leq \Size{\frac{a-x^*}{2}-\frac{b-x^*}{2}}\\
&= \frac{|b-a|}{2}.\end{align*}
Thus, in general \[\epsilon_n = |c-x^*| \leq \frac{|b-a|}{2},\] i.e. the error is at least halved each iteration, and the method converges linearly.
\begin{eg}
The positive zero of the polynomial $f(x)=x^2-2$ can be approximated using bisection with starting guesses $a=1$ and $b=2$. Then $f(1)=-1<0$ and $f(2)=2>0$. It follows (by continuity of $f$) that there is a root in the interval $[1,2]$. Then $f(c)=f(1.5)=0.25>0$. Thus, we replace $b=2$ with $c=1.5$. Continuing yields $1.25, 1.375, 1.4375, \ldots$. Eventually, we get an approximation for  $\sqrt 2$.
\end{eg}

\subsection{Newton's method}
A quicker alternative to bisection is Newton's method (also known as Newton-Raphson). Given an initial guess $x_0$ and a sufficiently nice derivative $f'$, we may estimate a zero $x^*$ of $f$.

Consider the Taylor expansion of $f$ around $x_n$ (see Appendix~\ref{sec:bigO} for the big $O$ notation and Appendix~\ref{sec:taylor} for Taylor): \[f(x)=f(x_n)+f'(x_n)(x-x_n)+O((x-x_n)^2).\] If we suppose that $x_n$ is close to the root $x^*$, the zero of the linear approximation $x_{n+1}$ is a good approximation for $x^*$ \[f(x_{n+1})\approx 0=f(x_n)+(x_{n+1}-x_n)f'(x_n).\] Rearranging, we arrive at the iterative formula for Newton's method \begin{equation}\label{eq:newtonStep} x_{n+1} = x_n-\frac{f(x_n)}{f'(x_n)}.\end{equation} This process is illustrated in Figure~\ref{fig:taylor}.

\begin{figure}\centering
	\includegraphics[width=0.7\textwidth]{example-image-golden}
	\caption{Newton's method}\label{fig:taylor}
\end{figure}

The signed error at iteration $n$ is $\epsilon_n = x_n-x^*$. By considering the quadratic term in the Taylor expansion around $x_n$, we get \begin{align*}f(x^*)&=f(x_n)+f'(x_n)(x^*-x_n)+\frac{f''(x_n)}{2}(x^*-x_n)^2 + O((x^*-x_n)^3)\\
\implies\quad& 0 = f(x_n) + f'(x_n)(x^*-x_n)+\frac{f''(x_n)}{2}\epsilon_n^2 + O(\epsilon_n^3)\\
\implies\quad& -\frac{f(x_n)}{f'(x_n)}=(x^*-x_n)+\frac{f''(x_n)}{2}\epsilon_n^2 + O(\epsilon_n^3)\\
\implies\quad& x_{n+1}-x_n=(x^*-x_n)+\frac{f''(x_n)}{2}\epsilon_n^2 + O(\epsilon_n^3)\\
 \implies\quad&\epsilon_{n+1}=\frac{f''(x_n)}{2}\epsilon_n^2 + O(\epsilon_n^3)
\end{align*}

Thus, as $n\to \infty$, $x_{n}\to x^*$ for a root $x^*$ of $f$. In particular, we have quadratic convergence.

\begin{eg}
The positive zero of the polynomial $f(x)=x^2-2$ can be approximated using Taylor's method with the starting guess $x_0=2$. Differentiating, $f'(x)=2x$. Then we get \begin{align*}
x_1 &= x_0 - \frac{f(x_0)}{f'(x_0)} = 2 - \frac{2^2-2}{4} = 1.5,\\
x_2 &= 1.5 - \frac{1.5^2-2}{3} = 1.41666667,\\
x_3 &= 1.41421569,\\
x_4 &= 1.41421356 \approx\sqrt{2}.
\end{align*}
This converges to $\sqrt{2}$ much faster than the bisection method as seen in Figure~\ref{fig:bisectionVsNewton}.
\begin{figure}\centering
	\includegraphics[width=0.7\linewidth]{figures/bisectionVsNewton}
	\caption{Bisection method vs Newton's method for approximating $\sqrt2$}\label{fig:bisectionVsNewton}
\end{figure}
\end{eg}

\begin{ex}
	Observe (in Python or otherwise) that approximating $\sqrt2$ with a bisection guess of $(1, 200)$ and Newton guess of $200$ yields similar log-linear error behaviour for small iteration step~$n$. Show that this is true by looking at Formula~\ref{eq:newtonStep}.
\end{ex}
Warning: if $f'(x_n)=0$, Newton's method will not work (division by zero!) -- pick a new~$x_0$. If the derivative is not well behaved (either not defined or close to zero at many points), then Newton's method may not be appropriate.


\subsection{Higher dimensions}
Consider the system of $m$ equations in $n$ variables $\ve{f}(\ve x)=\ve 0$ given by \[\begin{cases} f_1(x_1,x_2, \ldots, x_n) =0,\\f_2(x_1,x_2,\ldots, x_n)=0, \\\hfil\vdots\\ f_m(x_1,x_2,\ldots,x_n)=0.\end{cases}\]
The iterative step of Newton's method becomes \begin{equation}
	\label{eq:newtonDim}\ve x_{n+1}=\ve x_n-J(\ve x_n)\inv \ve{f}(\ve x_n),
\end{equation} where $J$ is the Jacobian matrix of $\mathbf f$ (an analogue to the derivative) given by $J_{ij}=\der{f_i}{x_j}$. This requires either matrix inversion (which is usually hard!) or solving a linear system (see Section~\ref{sec:numla}).

\begin{eg}
	Consider the steady state of the predator prey model: \[\begin{cases}	
	f_1(x,y)=Ax-Bxy=0,\\f_2(x,y)=Dxy-Cy=0.
	\end{cases}\]
	The Jacobian is \[J=\matrixtwo{\der{f_1}x,\der{f_1}y,\der{f_2}{x},\der{f_2}y}=\matrixtwo{A-By,-Bx,Dy,Dx-C}.	\]
	Let $\ve x_0=\matrixtwoone{2,1}$. Then \[\ve x_1 = \ve x_0 - J(\ve x_0)\inv \ve f(\ve x_n) = \matrixtwoone{2,1} - \matrixtwo{A-B,-2B,D,2D-C}\inv\matrixtwoone{2A-2B,2D-C}.\]
\end{eg}

\begin{note}[Useful commands]\
	\vspace{-2em}\begin{itemize}\itemsep0em
		\item Python SciPy's \lstinline|otimize.fsolve| finds the roots of a function.
		\item Python NumPy's \lstinline|linalg.solve| solves a linear system.
	\end{itemize}
\end{note}

\section{Finite difference}
\subsection{Discretising ODEs}
Consider the first order differential equation \[\dder {\ve y}t=\ve f({\ve y},t).\]
The Taylor expansion of ${\ve y}$ about time $t$ is \[\ve y(t+\dt)=\ve y(t)+ \dder {\ve y}t\dt+O((\dt)^2).\] Rearranging and dividing through by $\dt$ yields the first order accurate finite difference approximation \begin{equation}\label{eq:findifapp}\dder{\ve y}t=\frac{{\ve y}(t+dt)-{\ve y}(t)}{dt} + O(\dt).\end{equation}
\subsubsection{Forward Euler}
Let $t_{n+1}=t_n+\dt=t_0+n\dt$. Given $\ve y(t_n)$, we can step forward one time step by substituting Equation~\ref{eq:findifapp} into the differential equation \[\ve y(t_{n+1})=\ve y(t_n)+\ve f(\ve y(t_n),t_n)\dt +O((\dt)^2).\]
The local truncation error is then \[\tau_{n+1}=({\ve y(t_{n+1})-\ve y(t_n)})-\ve f(\ve y(t_n),t_n)\dt=O((\dt)^2).\] (Note that LeVeque calls $\frac{\tau_{n+1}}{\dt}$ the local truncation error.) Since the total number of steps is proportional to $\frac1{\dt}$, the global error is $\frac{\tau_{n+1}}{\dt}=O(\dt)$. This tells us that the forward Euler method is first order accurate. Since the next time step can be calculated directly from the information from the current time step, forward Euler is an explicit method.

\subsubsection{Backward Euler}
If we take a backward difference instead of a forward difference, we get \[\ve y(t_n)=\ve y(t_{n+1})-\ve f(\ve y(t_{n+1}),t_{n+1})\dt +O((\dt)^2).\] Since we cannot determine $\ve y(t_n)$ directly, backward Euler is implicit, and we must use a root finding algorithm.

\begin{eg}
Consider the IVP $\dder yt=-ky=f(y,t)$ with initial value $y(t_0)=1$. This can be integrated exactly and has an analytical solution of $y(t)=\exp(-k(t-t_0))$. Using forward Euler, \[y(t_1)=y(t_0)=f(y(t_0),t_0)\dt=1-k\dt.\] Similarly, for backward Euler, \[y(t_1)=y(t_0)+f(y(t_1),t_1)\dt = 1-ky(t_1)\dt \implies y(t_1)=\frac1{1+k\dt}.\]
\end{eg}

\begin{figure}\centering
	\includegraphics[width=0.7\textwidth]{example-image-golden}
	\caption{Euler}\label{fig:euler}
\end{figure}

\subsection{Central difference}
One way to reduce the truncation error from the first order forward and backward Euler schemes is to use more points in time, say \[y'\approx a	y_{n+1}+by+cy_{n-1},\] where $y=y(t_n)$, $y_{n+1}=y(t_{n+1})$, $y_{n-1}=y(t_{n-1})$ and $y' = \dder yt\eval{t_n}$. To find the best choices of $a,b,c$, we first Taylor expand \begin{align*}
y_{n+1}&=y+y'\dt+\frac{y''}{2}(\dt)^2+\frac{y''}{6}(\dt)^3+O((\dt)^4),\\
y_{n-1}&=y-y'\dt+\frac{y''}{2}(\dt)^2-\frac{y''}{6}(\dt)^3+O((\dt)^4).
\end{align*} It follows that \[ay_{n+1}+by+cy_{n-1}=(a+b+c)y+(a-c)y'\dt+(a+c)\frac{y''}{2}(\dt)^2 +(a-c)\frac{y'''}{6}(\dt^3)+O((\dt)^4).\]
We want $a+b+c=a+b=0$, i.e. $a=-c$ and $b=0$. Making the decision $(a-c)\dt=1$, we get $a=\frac1{2\dt}$ and $c=-\frac1{2\dt}$. The centred difference of $y'$ is thus \[\dder yt\Eval{t_n}=\frac{y_{n+1}-y_{n-1}}{2\dt}+O((\dt)^2)\] which is second order accurate.

If instead we took $a+b+c=a-c=0$ and $(a+c)\frac{(\dt)^2}{2}=1$, then $a=c=\frac1{(\dt)^2}$ and $b=\frac{-2}{(\dt)^2}$. The centred difference for the second derivative is \[\ddder yt\Eval{t_n}=\frac{y_{n+1}-2y_n+y_{n-1}}{2}+O((\dt)^2).\]

\subsection{Leap-frog}

\[\ve y_{n+1}=\ve y_{n-1}+2\ve f(\ve y_n,t_n)\dt+O((\dt)^3).\]

Needs forward Euler for the first step.


\begin{figure}\centering
	\includegraphics[width=0.7\textwidth]{example-image-golden}
	\caption{Leap-frog}\label{fig:leapFrog}
\end{figure}

Needs time filtering to stop the even/odd time iterations diverging.

\subsubsection{Robert-Asselin Filter}

\subsection{Mid-point}
Explicit mid-point

Implicit mid-point
\subsection{Trapezoidal}
(Also called Crank-Nicolson.)

\begin{eg}

\end{eg}

\subsection{Multi-step}

AB1 = Forward Euler

\subsection{Numerical stability}

\section{Von Neumann analysis}


\section{Numerical linear algebra}\label{sec:numla}


\appendix
\section{Background theory}
\subsection{\texorpdfstring{Big $O$}{Big O} notation}\label{sec:bigO}
\subsection{Taylor expansions}\label{sec:taylor}
\begin{thm}[Taylor]\label{thm:taylor}
	\[f(x)=f(x_0)+f'(x_0)(x-x_0) + \cdots\]
\end{thm}

\nocite{*}
\bibliographystyle{abbrv}
\bibliography{refs}
\addcontentsline{toc}{section}{References}

\label{lastpage}
\end{document}
