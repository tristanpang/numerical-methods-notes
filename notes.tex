\documentclass[11pt, a4paper]{article}

%% Packages
\usepackage{amsmath}
\usepackage{amsfonts,amsthm,amssymb,enumerate}
\usepackage{pgf,tikz}
\usepackage{graphicx}
\graphicspath{{../figures/}}
\usepackage[hidelinks]{hyperref}

%% Page style
\setlength{\parindent}{0pt}
\setlength\parskip{.7em plus 0.1em minus 0.2em}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\footrulewidth}{0.4pt}
\cfoot{Page \thepage\ of \pageref{lastpage}}
\rhead{Draft \textit{\number\day/\number\month/\number\year}}
\lhead{\leftmark}
\fancyheadoffset{0cm}

\usepackage[headsep=.5cm,headheight=14.5pt,margin=3cm]{geometry}
%\usepackage{showframe}

%% Shortcuts
\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\KK}{\mathbb{K}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\inv}{^{-1}}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\phi}{\varphi}

%% Theorems
\newtheoremstyle{break}
{\topsep}{\topsep}%
{}{}%
{\bfseries}{}%
{\newline}{}%

\theoremstyle{break}
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{propdefn}[thm]{Proposition/Definition}
\newtheorem{lemdefn}[thm]{Lemma/Definition}
\newtheorem{eg}[thm]{Example}
\newtheorem{ex}[thm]{Exercise}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{note}[thm]{Note}
\newtheorem{defn}[thm]{Definition}
\newtheorem{rmk}[thm]{Remark}
\newtheorem{re}[thm]{Results}
\newtheorem{code}[thm]{Code}
\newenvironment{prf}[1][\proofname]{%
	\begin{proof}[#1]$ $\par\nobreak\ignorespaces
	}{%
	\end{proof}
}

%% Magic
\makeatletter
\newcommand*{\perm}[1]{\text{$(\@for\tmp:=#1\do{{\tmp}\;}\hspace{-.277em})$}}%permutation
\makeatother
\makeatletter
\newcommand*{\permcom}[1]{\text{$(#1)$}}%permutation
\makeatother
\newcommand*{\ideal}[1]{\text{$\langle#1\rangle$}}%ideal
\newcommand*{\Ideal}[1]{\text{$\left\langle#1\right\rangle$}}%ideal
\def\matrixtwooneHelp(#1,#2){\begin{pmatrix}#1\\#2\end{pmatrix}}%matrix
\def\matrixtwoHelp(#1,#2,#3,#4){\begin{pmatrix}#1&#2\\#3&#4\end{pmatrix}}%matrix
\def\matrixthreeHelp(#1,#2,#3,#4,#5,#6,#7,#8,#9){\begin{pmatrix}#1&#2&#3\\#4&#5&#6\\#7&#8&#9 \end{pmatrix}}%matrix
\newcommand*{\matrixtwo}[1]{\matrixtwoHelp(#1)}
\newcommand*{\matrixtwoone}[1]{\matrixtwooneHelp(#1)}
\newcommand*{\matrixthree}[1]{\matrixthreeHelp(#1)}
\newcommand*{\size}[1]{|#1|}%size of group
\newcommand*{\Size}[1]{{\left|#1\right|}}%big size
\newcommand*{\ind}[2]{|#1:#2|}
\newcommand*{\set}[1]{\{#1\}}%set 
\newcommand*{\Set}[1]{\left\{#1\right\}}%set 
\newcommand*{\paren}[1]{(#1)}%set 
\newcommand*{\Paren}[1]{\left(#1\right)}%set 
\newcommand{\cross}{\times}
\newcommand{\rcross}{\rtimes}
\newcommand{\lcross}{\ltimes}
\newcommand{\id}{\mathsf{id}}


%% Specific commands
\newcommand{\ve}{\mathbf} % Vectors
\newcommand{\dt}{\Delta t}
\newcommand{\dx}{\Delta x}

\begin{document}
\begin{center}
{\LARGE Numerical Methods}\\
{\large Compiled by Tristan Pang}\\
{\the\month/\the\year}
\end{center}
These notes are based on the Oxford Numerical Methods course taught by David Marshall (2023 for the NERC DTP) with additional information from LeVeque \cite{leveque_finite_2007}.

These are rough notes only. A polished version may or may not be completed. Please direct all typos to me. The \href{https://github.com/tristanpang/numerical-methods-notes}{GitHub repo}\footnote{https://github.com/tristanpang/numerical-methods-notes} contains \LaTeX\ source, Python scripts for figures, and other useful Python things.

\tableofcontents



\section{Root finding}
Consider a sufficiently smooth function $f(x)$. If $f$ is a quadratic polynomial, we may find the zeros of $f$ using the quadratic formula, but for degrees 5 or larger, there exists no general formula for the zeros (Abelâ€“Ruffini theorem). In general, finding an $x^*$ such that $f(x^*)=0$ cannot be computed exactly. Instead one must employ numerical root finding algorithms. Common methods include the bisection method and Newton's method.

\subsection{Bisection method}
To find a zero $x^*$ of $f$, the bisection method takes two initial guesses $a$ and $b$ such that $a<0$ and $b\geq0$. IVT guarantees a zero between the two guesses. Calculate the midpoint \[c=\frac{a+b}2.\] If $f(c)<0$, replace $a$ with $c$; otherwise replace $b$ with $c$. Continue iterating until convergence is observed as shown in Figure~\ref{fig:bisection}.

\begin{figure}\centering
	\includegraphics[width=0.7\textwidth]{example-image-golden}
	\caption{Bisection method}\label{fig:bisection}
\end{figure}

The error at the first iteration is \begin{align*}
\epsilon_1 &= |c - x^*| \\&= \Size{\frac{a-x^*}{2}+\frac{b-x^*}{2}}\\&= \Bigg|\Size{\frac{a-x^*}{2}}-\Size{\frac{b-x^*}{2}}\Bigg| \\&\leq \Size{\frac{a-x^*}{2}-\frac{b-x^*}{2}}\\
&= \frac{|b-a|}{2}.\end{align*}
Thus, in general \[\epsilon_n = |c-x^*| \leq \frac{|b-a|}{2},\] i.e. the error is at least halved each iteration, and the method converges linearly.
\begin{eg}
The positive zero of the polynomial $f(x)-x^2-2$ can be approximated using bisection with starting guesses $1$ and $2$.

\end{eg}

\subsection{Newton's method}
A quicker alternative to bisection is Newton's method (also known as Newton-Raphson). Given an initial guess $x_0$ and a sufficiently nice derivative $f'$, we may estimate a zero $x^*$ of $f$.

Consider the Taylor expansion of $f$ around $x_n$ (see Appendix~\ref{sec:bigO} for the big $O$ notation and Appendix~\ref{sec:taylor} for Taylor): \[f(x)=f(x_n)+f'(x_n)(x-x_n)+O((x-x_n)^2).\] If we suppose that $x_n$ is close to the root $x^*$, the zero of the linear approximation $x_{n+1}$ is a good approximation for $x^*$ \[f(x_{n+1})\approx 0=f(x_n)+(x_{n+1}-x_n)f'(x_n).\] Rearranging, we arrive at the iterative formula for Newton's method \begin{equation} x_{n+1} = x_n-\frac{f(x_n)}{f'(x_n)}.\end{equation} This process is illustrated in Figure~\ref{fig:taylor}.

\begin{figure}\centering
	\includegraphics[width=0.7\textwidth]{example-image-golden}
	\caption{Newton's method}\label{fig:taylor}
\end{figure}

The signed error at iteration $n$ is $\epsilon_n = x_n-x^*$. By considering the quadratic term in the Taylor expansion around $x_n$, we get \begin{align*}f(x^*)&=f(x_n)+f'(x_n)(x^*-x_n)+\frac{f''(x_n)}{2}(x^*-x_n)^2 + O((x^*-x_n)^3)\\
\implies\quad& 0 = f(x_n) + f'(x_n)(x^*-x_n)+\frac{f''(x_n)}{2}\epsilon_n^2 + O(\epsilon_n^3)\\
\implies\quad& -\frac{f(x_n)}{f'(x_n)}=(x^*-x_n)+\frac{f''(x_n)}{2}\epsilon_n^2 + O(\epsilon_n^3)\\
\implies\quad& x_{n+1}-x_n=(x^*-x_n)+\frac{f''(x_n)}{2}\epsilon_n^2 + O(\epsilon_n^3)\\
 \implies\quad&\epsilon_{n+1}=\frac{f''(x_n)}{2}\epsilon_n^2 + O(\epsilon_n^3)
\end{align*}

Thus, as $n\to \infty$, $x_{n}\to x^*$ for a root $x^*$ of $f$. In particular, we have quadratic convergence.

\begin{eg}
The positive zero of the polynomial $f(x)=x^2-2$ can be approximated using Taylor's method with the starting guess $x_0=2$. Differentiating, $f'(x)=2x$. Then we get \begin{align*}
x_1 &= x_0 - \frac{f(x_0)}{f'(x_0)} = 2 - \frac{2^2-2}{4} = 1.5\\
x_2 &= 1.5 - \frac{1.5^2-2}{3} = 1.41666667\\
x_3 &= 1.41421569\\
x_4 &= 1.41421356 =\sqrt{2}.
\end{align*}
\end{eg}

Warning: if $f'(x_n)=0$, Newton's method will not work (division by zero!) -- pick a new~$x_0$. If the derivative is not well behaved (either not defined or close to zero at many points), then Newton's method may not be appropriate.


\subsection{Higher dimensions}
Consider the system of $m$ equations in $n$ variables $\ve{f}(\ve x)=\ve 0$ given by \[\begin{cases} f_1(x_1,x_2, \ldots, x_n) =0,\\f_2(x_1,x_2,\ldots, x_n)=0, \\\hfil\vdots\\ f_m(x_1,x_2,\ldots,x_n)=0.\end{cases}\]

\section{Finite difference}


\section{Von Neumann analysis}


\section{Numerical linear algebra}


\appendix
\section{Background theory}
\subsection{\texorpdfstring{Big $O$}{Big O} notation}\label{sec:bigO}
\subsection{Taylor expansions}\label{sec:taylor}
\begin{thm}[Taylor]\label{thm:taylor}
	\[f(x)=f(x_0)+f'(x_0)(x-x_0) + \cdots\]
\end{thm}

\nocite{*}
\bibliographystyle{abbrv}
\bibliography{refs}
\addcontentsline{toc}{section}{References}

\label{lastpage}
\end{document}
