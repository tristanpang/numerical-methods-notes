\documentclass[11pt, a4paper]{article}

%% Packages
\usepackage{amsmath}
\usepackage{amsfonts,amsthm,amssymb,enumerate}
\usepackage{pgf,tikz}
\usepackage{graphicx}
\graphicspath{{../figures/}}
\usepackage[hidelinks]{hyperref}

%% Page style
\setlength{\parindent}{0pt}
\setlength\parskip{.7em plus 0.1em minus 0.2em}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\footrulewidth}{0.4pt}
\cfoot{Page \thepage\ of \pageref{lastpage}}
\rhead{Draft \textit{\number\day/\number\month/\number\year}}
\lhead{\leftmark}
\fancyheadoffset{0cm}

\usepackage[headsep=.5cm,headheight=14.5pt,margin=3cm]{geometry}
%\usepackage{showframe}

%% Shortcuts
\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\KK}{\mathbb{K}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\inv}{^{-1}}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\phi}{\varphi}

%% Theorems
\newtheoremstyle{break}
{\topsep}{\topsep}%
{}{}%
{\bfseries}{}%
{\newline}{}%

\theoremstyle{break}
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{propdefn}[thm]{Proposition/Definition}
\newtheorem{lemdefn}[thm]{Lemma/Definition}
\newtheorem{eg}[thm]{Example}
\newtheorem{ex}[thm]{Exercise}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{note}[thm]{Note}
\newtheorem{defn}[thm]{Definition}
\newtheorem{rmk}[thm]{Remark}
\newtheorem{re}[thm]{Results}
\newtheorem{code}[thm]{Code}
\newenvironment{prf}[1][\proofname]{%
	\begin{proof}[#1]$ $\par\nobreak\ignorespaces
	}{%
	\end{proof}
}

%% Code
\usepackage{listings}
\lstset{language=Python,upquote=true, 
	numbers=left, stepnumber=1, 
	basicstyle=\small\ttfamily,columns=fullflexible,tabsize=5, xleftmargin=.25in, 	showstringspaces=false,breaklines=true,keepspaces}

%% Magic
\makeatletter
\newcommand*{\perm}[1]{\text{$(\@for\tmp:=#1\do{{\tmp}\;}\hspace{-.277em})$}}%permutation
\makeatother
\makeatletter
\newcommand*{\permcom}[1]{\text{$(#1)$}}%permutation
\makeatother
\newcommand*{\ideal}[1]{\text{$\langle#1\rangle$}}%ideal
\newcommand*{\Ideal}[1]{\text{$\left\langle#1\right\rangle$}}%ideal
\def\matrixtwooneHelp(#1,#2){\begin{pmatrix}#1\\#2\end{pmatrix}}%matrix
\def\matrixtwoHelp(#1,#2,#3,#4){\begin{pmatrix}#1&#2\\#3&#4\end{pmatrix}}%matrix
\def\matrixthreeHelp(#1,#2,#3,#4,#5,#6,#7,#8,#9){\begin{pmatrix}#1&#2&#3\\#4&#5&#6\\#7&#8&#9 \end{pmatrix}}%matrix
\newcommand*{\matrixtwo}[1]{\matrixtwoHelp(#1)}
\newcommand*{\matrixtwoone}[1]{\matrixtwooneHelp(#1)}
\newcommand*{\matrixthree}[1]{\matrixthreeHelp(#1)}
\newcommand*{\size}[1]{|#1|}%size of group
\newcommand*{\Size}[1]{{\left|#1\right|}}%big size
\newcommand*{\ind}[2]{|#1:#2|}
\newcommand*{\set}[1]{\{#1\}}%set 
\newcommand*{\Set}[1]{\left\{#1\right\}}%set 
\newcommand*{\paren}[1]{(#1)}%set 
\newcommand*{\Paren}[1]{\left(#1\right)}%set 
\newcommand{\cross}{\times}
\newcommand{\rcross}{\rtimes}
\newcommand{\lcross}{\ltimes}
\newcommand{\id}{\mathsf{id}}


%% Specific commands
\newcommand{\ve}{\mathbf} % Vectors
\newcommand{\dt}{\Delta t}
\newcommand{\dx}{\Delta x}
\newcommand{\der}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pder}[3][2]{\frac{\partial^#1 #2}{\partial #3^#1}}
\newcommand{\dder}[2]{\frac{d #1}{d #2}}
\newcommand{\ddder}[3][2]{\frac{d^#1 #2}{d #3^#1}}
\newcommand{\eval}[1]{\big\rvert_{#1}}
\newcommand{\Eval}[1]{\bigg\rvert_{#1}}
\newcommand{\dd}{\ \mathrm{d}}
\begin{document}
\begin{center}
{\LARGE Numerical Methods}\\
{\large Compiled by Tristan Pang}\\
{\the\month/\the\year}
\end{center}
These notes are based on the Oxford Numerical Methods course taught by David Marshall (2023 for the NERC DTP) with additional information from LeVeque \cite{leveque_finite_2007}.

These are rough notes only. A polished version may or may not be completed. Please direct all typos to me. The \href{https://github.com/tristanpang/numerical-methods-notes}{GitHub repo}\footnote{https://github.com/tristanpang/numerical-methods-notes} contains \LaTeX\ source, Python scripts for figures, and other useful Python things.

\tableofcontents



\section{Root finding}
Consider a sufficiently smooth function $f(x)$. If $f$ is a quadratic polynomial, we may find the zeros of $f$ using the quadratic formula, but for degrees 5 or larger, there exists no general formula for the zeros (Abelâ€“Ruffini theorem). In general, finding an $x^*$ such that $f(x^*)=0$ cannot be computed exactly. Instead one must employ numerical root finding algorithms. Common methods include the bisection method and Newton's method.

\subsection{Bisection method}
To find a zero $x^*$ of $f$, the bisection method takes two initial guesses $a$ and $b$ such that $a<0$ and $b\geq0$. IVT guarantees a zero between the two guesses. Calculate the midpoint \[c=\frac{a+b}2.\] If $f(c)<0$, replace $a$ with $c$; otherwise replace $b$ with $c$. Continue iterating until convergence is observed as shown in Figure~\ref{fig:bisection}.

\begin{figure}\centering
	\includegraphics[width=0.7\textwidth]{example-image-golden}
	\caption{Bisection method}\label{fig:bisection}
\end{figure}

The error at the first iteration is \begin{align*}
\epsilon_1 &= |c - x^*| \\&= \Size{\frac{a-x^*}{2}+\frac{b-x^*}{2}}\\&= \Bigg|\Size{\frac{a-x^*}{2}}-\Size{\frac{b-x^*}{2}}\Bigg| \\&\leq \Size{\frac{a-x^*}{2}-\frac{b-x^*}{2}}\\
&= \frac{|b-a|}{2}.\end{align*}
Thus, in general \[\epsilon_n = |c-x^*| \leq \frac{|b-a|}{2},\] i.e. the error is at least halved each iteration, and the method converges linearly.
\begin{eg}
The positive zero of the polynomial $f(x)=x^2-2$ can be approximated using bisection with starting guesses $a=1$ and $b=2$. Then $f(1)=-1<0$ and $f(2)=2>0$. It follows (by continuity of $f$) that there is a root in the interval $[1,2]$. Then $f(c)=f(1.5)=0.25>0$. Thus, we replace $b=2$ with $c=1.5$. Continuing yields $1.25, 1.375, 1.4375, \ldots$. Eventually, we get an approximation for  $\sqrt 2$.
\end{eg}

\subsection{Newton's method}
A quicker alternative to bisection is Newton's method (also known as Newton-Raphson). Given an initial guess $x_0$ and a sufficiently nice derivative $f'$, we may estimate a zero $x^*$ of $f$.

Consider the Taylor expansion of $f$ around $x_n$ (see Appendix~\ref{sec:bigO} for the big $O$ notation and Appendix~\ref{sec:taylor} for Taylor): \[f(x)=f(x_n)+f'(x_n)(x-x_n)+O((x-x_n)^2).\] If we suppose that $x_n$ is close to the root $x^*$, the zero of the linear approximation $x_{n+1}$ is a good approximation for $x^*$ \[f(x_{n+1})\approx 0=f(x_n)+(x_{n+1}-x_n)f'(x_n).\] Rearranging, we arrive at the iterative formula for Newton's method \begin{equation}\label{eq:newtonStep} x_{n+1} = x_n-\frac{f(x_n)}{f'(x_n)}.\end{equation} This process is illustrated in Figure~\ref{fig:taylor}.

\begin{figure}\centering
	\includegraphics[width=0.7\textwidth]{example-image-golden}
	\caption{Newton's method}\label{fig:taylor}
\end{figure}

The signed error at iteration $n$ is $\epsilon_n = x_n-x^*$. By considering the quadratic term in the Taylor expansion around $x_n$, we get \begin{align*}f(x^*)&=f(x_n)+f'(x_n)(x^*-x_n)+\frac{f''(x_n)}{2}(x^*-x_n)^2 + O((x^*-x_n)^3)\\
\implies\quad& 0 = f(x_n) + f'(x_n)(x^*-x_n)+\frac{f''(x_n)}{2}\epsilon_n^2 + O(\epsilon_n^3)\\
\implies\quad& -\frac{f(x_n)}{f'(x_n)}=(x^*-x_n)+\frac{f''(x_n)}{2}\epsilon_n^2 + O(\epsilon_n^3)\\
\implies\quad& x_{n+1}-x_n=(x^*-x_n)+\frac{f''(x_n)}{2}\epsilon_n^2 + O(\epsilon_n^3)\\
 \implies\quad&\epsilon_{n+1}=\frac{f''(x_n)}{2}\epsilon_n^2 + O(\epsilon_n^3)
\end{align*}

Thus, as $n\to \infty$, $x_{n}\to x^*$ for a root $x^*$ of $f$. In particular, we have quadratic convergence.

\begin{eg}
The positive zero of the polynomial $f(x)=x^2-2$ can be approximated using Taylor's method with the starting guess $x_0=2$. Differentiating, $f'(x)=2x$. Then we get \begin{align*}
x_1 &= x_0 - \frac{f(x_0)}{f'(x_0)} = 2 - \frac{2^2-2}{4} = 1.5,\\
x_2 &= 1.5 - \frac{1.5^2-2}{3} = 1.41666667,\\
x_3 &= 1.41421569,\\
x_4 &= 1.41421356 \approx\sqrt{2}.
\end{align*}
This converges to $\sqrt{2}$ much faster than the bisection method as seen in Figure~\ref{fig:bisectionVsNewton}.
\begin{figure}\centering
	\includegraphics[width=0.7\linewidth]{figures/bisectionVsNewton}
	\caption{Bisection method vs Newton's method for approximating $\sqrt2$}\label{fig:bisectionVsNewton}
\end{figure}
\end{eg}

\begin{ex}
	Observe (in Python or otherwise) that approximating $\sqrt2$ with a bisection guess of $(1, 200)$ and Newton guess of $200$ yields similar log-linear error behaviour for small iteration step~$n$. Show that this is true by looking at Formula~\ref{eq:newtonStep}.
\end{ex}
Warning: if $f'(x_n)=0$, Newton's method will not work (division by zero!) -- pick a new~$x_0$. If the derivative is not well behaved (either not defined or close to zero at many points), then Newton's method may not be appropriate.


\subsection{Higher dimensions}
Consider the system of $m$ equations in $n$ variables $\ve{f}(\ve x)=\ve 0$ given by \[\begin{cases} f_1(x_1,x_2, \ldots, x_n) =0,\\f_2(x_1,x_2,\ldots, x_n)=0, \\\hfil\vdots\\ f_m(x_1,x_2,\ldots,x_n)=0.\end{cases}\]
The iterative step of Newton's method becomes \begin{equation}
	\label{eq:newtonDim}\ve x_{n+1}=\ve x_n-J(\ve x_n)\inv \ve{f}(\ve x_n),
\end{equation} where $J$ is the Jacobian matrix of $\mathbf f$ (an analogue to the derivative) given by $J_{ij}=\der{f_i}{x_j}$. This requires either matrix inversion (which is usually hard!) or solving a linear system (see Section~\ref{sec:numla}).

\begin{eg}
	Consider the steady state of the predator prey model: \[\begin{cases}	
	f_1(x,y)=Ax-Bxy=0,\\f_2(x,y)=Dxy-Cy=0.
	\end{cases}\]
	The Jacobian is \[J=\matrixtwo{\der{f_1}x,\der{f_1}y,\der{f_2}{x},\der{f_2}y}=\matrixtwo{A-By,-Bx,Dy,Dx-C}.	\]
	Let $\ve x_0=\matrixtwoone{2,1}$. Then \[\ve x_1 = \ve x_0 - J(\ve x_0)\inv \ve f(\ve x_n) = \matrixtwoone{2,1} - \matrixtwo{A-B,-2B,D,2D-C}\inv\matrixtwoone{2A-2B,2D-C}.\]
\end{eg}

\begin{note}[Useful commands]\
	\vspace{-2em}\begin{itemize}\itemsep0em
		\item Python SciPy's \lstinline|otimize.fsolve| finds the roots of a function.
		\item Python NumPy's \lstinline|linalg.solve| solves a linear system.
	\end{itemize}
\end{note}

\section{Finite difference}
\subsection{Discretising ODEs}
Consider the first order differential equation \[\dder {\ve y}t=\ve f({\ve y},t).\]
The Taylor expansion of ${\ve y}$ about time $t$ is \[\ve y(t+\dt)=\ve y(t)+ \dder {\ve y}t\dt+O((\dt)^2).\] Rearranging and dividing through by $\dt$ yields the first order accurate finite difference approximation \begin{equation}\label{eq:findifapp}\dder{\ve y}t=\frac{{\ve y}(t+dt)-{\ve y}(t)}{dt} + O(\dt).\end{equation}
\subsubsection{Forward Euler}
Let $t_{n+1}=t_n+\dt=t_0+n\dt$. Given $\ve y(t_n)$, we can step forward one time step by substituting Equation~\ref{eq:findifapp} into the differential equation \[\ve y(t_{n+1})=\ve y(t_n)+\ve f(\ve y(t_n),t_n)\dt +O((\dt)^2).\]
The local truncation error is then \[\tau_{n+1}=({\ve y(t_{n+1})-\ve y(t_n)})-\ve f(\ve y(t_n),t_n)\dt=O((\dt)^2).\] (Note that LeVeque calls $\frac{\tau_{n+1}}{\dt}$ the local truncation error.) Since the total number of steps is proportional to $\frac1{\dt}$, the global error is $\frac{\tau_{n+1}}{\dt}=O(\dt)$. This tells us that the forward Euler method is first order accurate. Since the next time step can be calculated directly from the information from the current time step, forward Euler is an explicit method.

\subsubsection{Backward Euler}
If we take a backward difference instead of a forward difference, we get \[\ve y(t_n)=\ve y(t_{n+1})-\ve f(\ve y(t_{n+1}),t_{n+1})\dt +O((\dt)^2).\] Since we cannot determine $\ve y(t_n)$ directly, backward Euler is implicit, and we must use a root finding algorithm.

\begin{eg}
Consider the IVP $\dder yt=-ky=f(y,t)$ with initial value $y(t_0)=1$. This can be integrated exactly and has an analytical solution of $y(t)=\exp(-k(t-t_0))$. Using forward Euler, \[y(t_1)=y(t_0)=f(y(t_0),t_0)\dt=1-k\dt.\] Similarly, for backward Euler, \[y(t_1)=y(t_0)+f(y(t_1),t_1)\dt = 1-ky(t_1)\dt \implies y(t_1)=\frac1{1+k\dt}.\]
\end{eg}

\begin{figure}\centering
	\includegraphics[width=0.7\textwidth]{example-image-golden}
	\caption{Euler}\label{fig:euler}
\end{figure}

\subsection{Central difference}
One way to reduce the truncation error from the first order forward and backward Euler schemes is to use more points in time, say \[y'\approx a	y_{n+1}+by+cy_{n-1},\] where $y=y(t_n)$, $y_{n+1}=y(t_{n+1})$, $y_{n-1}=y(t_{n-1})$ and $y' = \dder yt\eval{t_n}$. To find the best choices of $a,b,c$, we first Taylor expand \begin{align*}
y_{n+1}&=y+y'\dt+\frac{y''}{2}(\dt)^2+\frac{y''}{6}(\dt)^3+O((\dt)^4),\\
y_{n-1}&=y-y'\dt+\frac{y''}{2}(\dt)^2-\frac{y''}{6}(\dt)^3+O((\dt)^4).
\end{align*} It follows that \[ay_{n+1}+by+cy_{n-1}=(a+b+c)y+(a-c)y'\dt+(a+c)\frac{y''}{2}(\dt)^2 +(a-c)\frac{y'''}{6}(\dt^3)+O((\dt)^4).\]
We want $a+b+c=a+b=0$, i.e. $a=-c$ and $b=0$. Making the decision $(a-c)\dt=1$, we get $a=\frac1{2\dt}$ and $c=-\frac1{2\dt}$. The centred difference of $y'$ is thus \[\dder yt\Eval{t_n}=\frac{y_{n+1}-y_{n-1}}{2\dt}+O((\dt)^2)\] which is second order accurate.

If instead we took $a+b+c=a-c=0$ and $(a+c)\frac{(\dt)^2}{2}=1$, then $a=c=\frac1{(\dt)^2}$ and $b=\frac{-2}{(\dt)^2}$. The centred difference for the second derivative is \[\ddder yt\Eval{t_n}=\frac{y_{n+1}-2y_n+y_{n-1}}{2}+O((\dt)^2).\]

\subsection{Leap-frog}

\[\ve y_{n+1}=\ve y_{n-1}+2\ve f(\ve y_n,t_n)\dt+O((\dt)^3).\]

Needs forward Euler for the first step.


\begin{figure}\centering
	\includegraphics[width=0.7\textwidth]{example-image-golden}
	\caption{Leap-frog}\label{fig:leapFrog}
\end{figure}

Needs time filtering to stop the even/odd time iterations diverging.

\subsubsection{Robert-Asselin Filter}
\subsection{Other methods}
\subsubsection{Mid-point}
Explicit mid-point takes a half forward Euler step before performing the full step
\[\ve y_{n+1}=\ve y_n + \ve f\Paren{\ve y_n+f(\ve y_n,t_n)\frac{\dt}2,t_{n+\frac12}}\dt.\]

Implicit mid-point is \[y_{n+1}	=y_n+f\Paren{\frac{y_n+y_{n+1}}{2},t_{n+\frac12}}\dt.\]
\subsubsection{Trapezoidal}
The trapezoidal method (also called Crank-Nicolson) is a combination of forward and backward Euler \[y_{n+1}=y_n+\Paren{\frac{f(y_n,t_n)+f(y_{n+1},t_{n+1})}{2}}\dt.\]

\begin{eg}
Consider the ODE $\dder yt=-kt$ with $y(t_0)=1$. The explicit mid-point gives \[y_1=-k\dt+\frac{k}{2}(\dt)^2.\] Since the ODE is linear, the trapezoidal method is equivalent to implicit mid-point \[y_1=\frac{1-\frac k2\dt}{1+\frac k2\dt}.\]
\end{eg}

\subsection{Multi-step}
The typical family of multi-step methods are the Adam-Bashforth methods. These are explicit. The key idea is to fit a polynomial to $f(t)$ and integrate over the $t$-step.
\subsubsection{AB2}
For convenience, set $t=0$ at $y_n$. Then \[y_{n+1}=y_n+\int_0^\dt f(y_n,t)\dd t\]
 We use the points at $n$ and $n+1$ to fit a linear polynomial to $f$, i.e. $f(t)=at+b$.
\subsubsection{AB3}

Higher order methods can be derived in a similar way. Note that AB1 is forward Euler.

\subsection{Numerical stability}

\section{Partial differential equations}
	A general linear second order PDE in two variables is given by \[a\pder\phi{x_1}+b\frac{\partial^2\phi}{\partial x_1\partial x_2}+c\pder\phi{x_2}+d\der\phi{x_1}+e\der\phi{x_2}+f\phi = g.\]
The discriminant is \[\Delta = b^2-4ac \implies \begin{cases}
	\Delta<0 & \text{elliptic},\\ \Delta =0& \text{parabolic}, \\\Delta>0 &\text{hyperbolic}.
\end{cases}\]
In general, \[\sum_{ij}A_{ij}\frac{\partial^2\phi}{\partial x_i\partial x_j}+\sum_i B_i\der\phi{x_i}+C\phi = D.\]

This PDE is called \emph{elliptic} if none of the eigenvalues vanish and all have the same sign; \emph{parabolic} if ne eigenvalue vanishes and the remainder have the same sign; \emph{hyperbolic} if none of the eigenvalues vanish and one has the opposite sign.

\subsection{Parabolic PDEs}

Consider the diffusion equation \[\der\phi t=\kappa\pder\phi x.\] This is a parabolic second order PDE. We need boundary conditions...

Diffusing equations have causality, i.e. the direction of time matters. Information spreads on scales $\delta\sim\sqrt{\kappa t}$ as in Figure~\ref{fig:diffusionTopHat} - the time step cannot be larger, otherwise the scheme becomes unstable.

\begin{figure}\centering
	\includegraphics[width=0.7\textwidth]{example-image-golden}
	\caption{Diffusion spreading}\label{fig:diffusionTopHat}
\end{figure}

\begin{eg}
	Consider diffusion equation on $0\leq x\leq L$ and $0\leq t\leq T$.  Let $\phi_j^n=\phi(x_j,t_n)$ where $t_n=n\dt$ and $x_j=j\dx$. The domain is discretised in Figure~\ref{fig:diffusionDiscret}.
	
	\begin{figure}\centering
		\includegraphics[width=0.7\textwidth]{example-image-golden}
		\caption{Discretising the diffusion equation domain}\label{fig:diffusionDiscret}
	\end{figure}
	
	Using forward Euler, \[\frac{\phi_j^{n+1}-\phi_j^n}{\dt}=\kappa\frac{\phi_{j+1}^n-2\phi_j^n+\phi^n_{j-1}}{(\dx)^2}.\] It follows that \[\phi_j^{n+1}=\phi_j^n+\frac{\kappa\dt}{(\dx)^2}\Paren{\phi_{j+1}^n-2\phi_n^n+\phi_{j-1}^n}.\] This is straightforward to solve as forward Euler explicit. On the other hand, implicit backward Euler is \[\frac{\phi_j^{n+1}-\phi_j^n}{\dt}=\kappa\frac{\phi_{j+1}^{n+1}-2\phi_j^{n+1}+\phi^{n+1}_{j-1}}{(\dx)^2}.\] There are three unknowns, so we can not solve this directly. We solve this in Section~\ref{sec:elliptic}.

\end{eg}

\subsection{Boundary conditions}
A Dirichlet boundary condition is when we specify $\phi$ on solid boundaries. For example, $\phi(0,t)=1$ or $\phi(L,t)=0.5\sin(2\pi t)$.

Neumann boundary conditions specify the normal derivative. For example, $\der\phi x(0,t)=0$ (no flux), $\der\phi x(L,t)=F$ (specified flux).

We can mix the boundary conditions (Robin boundary conditions) to get $a\phi+b\der\phi x=c$.

Dirichlet boundary conditions are easy to implement by setting $\phi$ at the boundary. Neumann boundary conditions are a bit more complicated. We can use a 1-sided derivative with a grid point on te boundary: \[\der\phi x(0,t)=0\implies \frac{\phi_1^n-\phi_0^n}{\dx}=0\implies \phi_0^n=\phi_1^n.\] This is only first order accurate. A better place for the boundary is between two midpoints, which results in a second order accurate centred derivative. This can be implemented in two ways. The first is using ``ghost points'', i.e. put $\phi_0$ outside the domain. This may not be appropriate for complicated domains. The second way is to absorb the boundary conditions into the finite difference operator. For example, if $\kappa \der\phi x\eval{0.5}^n=0$, \[\kappa \pder\phi x\Eval1^n=\der{}x\Paren{\kappa\der\phi x}\Eval 1^n=\frac{\kappa\der\phi x\eval{1.5}^n-\kappa \der\phi x\eval{0.5}^n}{\dx}=\frac{\kappa}{\dx}(\phi_2^n-\phi_1^n).\]


\section{Von Neumann analysis}


\section{Numerical linear algebra}\label{sec:numla}


\appendix
\section{Background theory}
\subsection{\texorpdfstring{Big $O$}{Big O} notation}\label{sec:bigO}
\subsection{Taylor expansions}\label{sec:taylor}
\begin{thm}[Taylor]\label{thm:taylor}
	\[f(x)=f(x_0)+f'(x_0)(x-x_0) + \cdots\]
\end{thm}

\nocite{*}
\bibliographystyle{abbrv}
\bibliography{refs}
\addcontentsline{toc}{section}{References}

\label{lastpage}
\end{document}
